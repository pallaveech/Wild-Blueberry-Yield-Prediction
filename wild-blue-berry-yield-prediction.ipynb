{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.feature_selection import mutual_info_regression, SelectKBest\nfrom sklearn.cluster import KMeans\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler\nfrom sklearn.model_selection import train_test_split, cross_val_score, KFold, GridSearchCV, RepeatedKFold\nfrom sklearn.ensemble import AdaBoostRegressor, GradientBoostingRegressor, RandomForestRegressor\nfrom sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\nimport sklearn\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.model_selection import GridSearchCV\nimport statsmodels.api as sm\nfrom xgboost import XGBRegressor\nimport xgboost\nimport shap\nimport joblib\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-12-18T13:14:21.579957Z","iopub.execute_input":"2022-12-18T13:14:21.580399Z","iopub.status.idle":"2022-12-18T13:14:21.598626Z","shell.execute_reply.started":"2022-12-18T13:14:21.580367Z","shell.execute_reply":"2022-12-18T13:14:21.596478Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"/kaggle/input/wildblueberrydatasetpollinationsimulation/WildBlueberryPollinationSimulationData.csv\n","output_type":"stream"}]},{"cell_type":"code","source":"xgboost.__version__","metadata":{"execution":{"iopub.status.busy":"2022-12-18T13:14:23.533949Z","iopub.execute_input":"2022-12-18T13:14:23.534481Z","iopub.status.idle":"2022-12-18T13:14:23.544467Z","shell.execute_reply.started":"2022-12-18T13:14:23.534442Z","shell.execute_reply":"2022-12-18T13:14:23.543239Z"},"trusted":true},"execution_count":4,"outputs":[{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"'1.6.2'"},"metadata":{}}]},{"cell_type":"code","source":"df = pd.read_csv(\"/kaggle/input/wildblueberrydatasetpollinationsimulation/WildBlueberryPollinationSimulationData.csv\", index_col='Row#')\ndf.head()","metadata":{"execution":{"iopub.status.busy":"2022-12-14T15:10:33.638035Z","iopub.execute_input":"2022-12-14T15:10:33.638573Z","iopub.status.idle":"2022-12-14T15:10:33.691947Z","shell.execute_reply.started":"2022-12-14T15:10:33.638529Z","shell.execute_reply":"2022-12-14T15:10:33.690781Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.info()","metadata":{"execution":{"iopub.status.busy":"2022-12-14T15:10:33.693993Z","iopub.execute_input":"2022-12-14T15:10:33.694748Z","iopub.status.idle":"2022-12-14T15:10:33.714316Z","shell.execute_reply.started":"2022-12-14T15:10:33.694699Z","shell.execute_reply":"2022-12-14T15:10:33.712858Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.describe()","metadata":{"execution":{"iopub.status.busy":"2022-12-14T15:10:33.717763Z","iopub.execute_input":"2022-12-14T15:10:33.718482Z","iopub.status.idle":"2022-12-14T15:10:33.796416Z","shell.execute_reply.started":"2022-12-14T15:10:33.718434Z","shell.execute_reply":"2022-12-14T15:10:33.794620Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"features_df = df.drop('yield', axis=1)\ntar = df['yield']","metadata":{"execution":{"iopub.status.busy":"2022-12-14T15:10:33.798728Z","iopub.execute_input":"2022-12-14T15:10:33.799717Z","iopub.status.idle":"2022-12-14T15:10:33.807127Z","shell.execute_reply.started":"2022-12-14T15:10:33.799664Z","shell.execute_reply":"2022-12-14T15:10:33.805876Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(15,15))\nsns.heatmap(df.corr(), annot=True, vmin=-1, vmax=1)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-12-14T15:10:33.808692Z","iopub.execute_input":"2022-12-14T15:10:33.809455Z","iopub.status.idle":"2022-12-14T15:10:35.475655Z","shell.execute_reply.started":"2022-12-14T15:10:33.809416Z","shell.execute_reply":"2022-12-14T15:10:35.474199Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(5,5))\nsns.boxplot(x='yield', data=df)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-12-14T15:10:35.477537Z","iopub.execute_input":"2022-12-14T15:10:35.478376Z","iopub.status.idle":"2022-12-14T15:10:35.669071Z","shell.execute_reply.started":"2022-12-14T15:10:35.478327Z","shell.execute_reply":"2022-12-14T15:10:35.667794Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.pairplot(df, \n            height=4,\n            plot_kws = {'alpha': 0.4, 's': 30, 'edgecolor': 'k'},\n            corner=True\n           );","metadata":{"execution":{"iopub.status.busy":"2022-12-14T15:10:35.673201Z","iopub.execute_input":"2022-12-14T15:10:35.673980Z","iopub.status.idle":"2022-12-14T15:11:16.176381Z","shell.execute_reply.started":"2022-12-14T15:10:35.673916Z","shell.execute_reply":"2022-12-14T15:11:16.175091Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.nunique()","metadata":{"execution":{"iopub.status.busy":"2022-12-14T15:11:16.177861Z","iopub.execute_input":"2022-12-14T15:11:16.178309Z","iopub.status.idle":"2022-12-14T15:11:16.191554Z","shell.execute_reply.started":"2022-12-14T15:11:16.178276Z","shell.execute_reply":"2022-12-14T15:11:16.190494Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"nominal_df = df[['MaxOfUpperTRange','MinOfUpperTRange','AverageOfUpperTRange','MaxOfLowerTRange',\n               'MinOfLowerTRange','AverageOfLowerTRange','RainingDays','AverageRainingDays']]\n\nfig, ax = plt.subplots(2,4, figsize=(20,13))\nfor e, col in enumerate(nominal_df.columns):\n    if e<=3:\n        sns.boxplot(data=df, x=col, y='yield', ax=ax[0,e])\n    else:\n        sns.boxplot(data=df, x=col, y='yield', ax=ax[1,e-4])       \nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-12-14T15:11:16.198027Z","iopub.execute_input":"2022-12-14T15:11:16.198461Z","iopub.status.idle":"2022-12-14T15:11:18.271055Z","shell.execute_reply.started":"2022-12-14T15:11:16.198424Z","shell.execute_reply":"2022-12-14T15:11:18.269559Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(15,10))\nplt.subplot(2,3,1)\nplt.hist(df['bumbles'])\nplt.title(\"Histogram of bumbles column\")\nplt.subplot(2,3,2)\nplt.hist(df['andrena'])\nplt.title(\"Histogram of andrena column\")\nplt.subplot(2,3,3)\nplt.hist(df['osmia'])\nplt.title(\"Histogram of osmia column\")\nplt.subplot(2,3,4)\nplt.hist(df['clonesize'])\nplt.title(\"Histogram of clonesize column\")\nplt.subplot(2,3,5)\nplt.hist(df['honeybee'])\nplt.title(\"Histogram of honeybee column\")\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-12-14T15:11:18.272873Z","iopub.execute_input":"2022-12-14T15:11:18.273654Z","iopub.status.idle":"2022-12-14T15:11:18.988889Z","shell.execute_reply.started":"2022-12-14T15:11:18.273618Z","shell.execute_reply":"2022-12-14T15:11:18.987510Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Observations:\n\n- Upper and lower T range columns correlate with each other\n- Rainy days and average rainy days correlates with each other\n- Fruitmass, fruitset and seeds are correlated\n- 'bumbles' column is highly imbalance while 'andrena' and 'osmia' columns are not\n- 'honeybee' is also imbalanced column compared to 'clonesize'","metadata":{}},{"cell_type":"code","source":"# run the MI scores of the dataset\nmi_score = mutual_info_regression(features_df, tar, n_neighbors=3,random_state=42)\nmi_score_df = pd.DataFrame({'columns':features_df.columns, 'MI_score':mi_score})\nmi_score_df.sort_values(by='MI_score', ascending=False)","metadata":{"execution":{"iopub.status.busy":"2022-12-14T15:11:18.990111Z","iopub.execute_input":"2022-12-14T15:11:18.990443Z","iopub.status.idle":"2022-12-14T15:11:19.092052Z","shell.execute_reply.started":"2022-12-14T15:11:18.990411Z","shell.execute_reply":"2022-12-14T15:11:19.090820Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- We can see TOP features with high correlation with target variable, we can keep most significant features and remove less significant features for modeling task\n- We will keep types of bees columns + clonesize and averageoflower and upper T range features for modeling\n- Cluster all 4 types of Bees to reduce features\n- standardize the dataset and build baseline using GBT and RandomForest\n- We will remove fruitset, fruitmass and seeds columns as they are part of Target only (Domain Knowledge insight)","metadata":{}},{"cell_type":"code","source":"# clustering using kmeans algorithm\nX_clus = features_df[['honeybee','osmia','bumbles','andrena']]\n\nscaler = StandardScaler()\nscaler.fit(X_clus)\nX_new_clus = scaler.transform(X_clus)\n\nclustering = KMeans(n_clusters=3, random_state=42)\nclustering.fit(X_new_clus)\nn_cluster = clustering.labels_","metadata":{"execution":{"iopub.status.busy":"2022-12-14T15:11:19.093710Z","iopub.execute_input":"2022-12-14T15:11:19.094138Z","iopub.status.idle":"2022-12-14T15:11:19.168461Z","shell.execute_reply.started":"2022-12-14T15:11:19.094095Z","shell.execute_reply":"2022-12-14T15:11:19.167528Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# add new feature to feature_Df \nfeatures_df['n_cluster'] = n_cluster\ndf['n_cluster'] = n_cluster\nfeatures_df['n_cluster'].value_counts()","metadata":{"execution":{"iopub.status.busy":"2022-12-14T15:11:19.170123Z","iopub.execute_input":"2022-12-14T15:11:19.175098Z","iopub.status.idle":"2022-12-14T15:11:19.190747Z","shell.execute_reply.started":"2022-12-14T15:11:19.175038Z","shell.execute_reply":"2022-12-14T15:11:19.189801Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# let's plot most imporatant feature VS yield\nplt.figure(figsize=(6,6))\nsns.scatterplot(x='seeds', y='yield', hue='n_cluster', data=df)\nplt.title(\"Clustering scatter plot\")\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-12-14T15:11:19.193140Z","iopub.execute_input":"2022-12-14T15:11:19.193633Z","iopub.status.idle":"2022-12-14T15:11:19.522392Z","shell.execute_reply.started":"2022-12-14T15:11:19.193589Z","shell.execute_reply":"2022-12-14T15:11:19.521142Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- Clustering helps to divide samples into three group and it can be seen 'yield' VS 'seeds' plot\n- We will reduce the feature set to select best features from our EDA proceed to build baseline modelling","metadata":{}},{"cell_type":"code","source":"features_set = ['AverageRainingDays','clonesize','AverageOfLowerTRange',\n               'AverageOfUpperTRange','honeybee','osmia','bumbles','andrena','n_cluster']\n\n# final dataframe\nX = features_df[features_set]\ny = tar.round(1)","metadata":{"execution":{"iopub.status.busy":"2022-12-14T15:11:19.523803Z","iopub.execute_input":"2022-12-14T15:11:19.524263Z","iopub.status.idle":"2022-12-14T15:11:19.532609Z","shell.execute_reply.started":"2022-12-14T15:11:19.524180Z","shell.execute_reply":"2022-12-14T15:11:19.531232Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# train and test dataset to build baseline model using GBT and RFs by scaling the dataset\nmx_scaler = MinMaxScaler()\nX_scaled = pd.DataFrame(mx_scaler.fit_transform(X))\nX_scaled.columns = X.columns","metadata":{"execution":{"iopub.status.busy":"2022-12-14T15:11:19.533994Z","iopub.execute_input":"2022-12-14T15:11:19.534388Z","iopub.status.idle":"2022-12-14T15:11:19.548194Z","shell.execute_reply.started":"2022-12-14T15:11:19.534343Z","shell.execute_reply":"2022-12-14T15:11:19.547208Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Baseline","metadata":{}},{"cell_type":"code","source":"# let's fit the data to the models\nmodel_dict = {\"abr\": AdaBoostRegressor(), \n              \"gbr\": GradientBoostingRegressor(), \n              \"rfr\": RandomForestRegressor()\n             }\n\nfor key, val in model_dict.items():\n    print(f\"cross validation for {key}\")\n    score = cross_val_score(val, X_scaled, y, cv=5, scoring='neg_mean_squared_error')\n    mean_score = -np.sum(score)/5\n    sqrt_score = np.sqrt(mean_score) \n    print(sqrt_score)","metadata":{"execution":{"iopub.status.busy":"2022-12-14T15:11:19.549286Z","iopub.execute_input":"2022-12-14T15:11:19.550526Z","iopub.status.idle":"2022-12-14T15:11:21.451693Z","shell.execute_reply.started":"2022-12-14T15:11:19.550486Z","shell.execute_reply":"2022-12-14T15:11:21.450442Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- lowest score is for adaboost regressor","metadata":{}},{"cell_type":"markdown","source":"## GBM modeling 1st iteration","metadata":{}},{"cell_type":"code","source":"# split the train and test data\nX_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n\nbgt = GradientBoostingRegressor(random_state=42)\nbgt.fit(X_train,y_train)\npreds = bgt.predict(X_test)\nscore = bgt.score(X_train,y_train)\nrmse_score = np.sqrt(mean_squared_error(y_test, preds))\nr2_score = r2_score(y_test, preds)\nprint(\"RMSE score gradient boosting machine:\", rmse_score)      \nprint(\"R2 score for the model: \", r2_score)","metadata":{"execution":{"iopub.status.busy":"2022-12-14T15:11:21.452920Z","iopub.execute_input":"2022-12-14T15:11:21.453239Z","iopub.status.idle":"2022-12-14T15:11:21.539313Z","shell.execute_reply.started":"2022-12-14T15:11:21.453210Z","shell.execute_reply":"2022-12-14T15:11:21.538038Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Hyper parameter tuning using gridsearchCV","metadata":{}},{"cell_type":"code","source":"kf = KFold(n_splits = 5, shuffle=True, random_state=0)\n\nparam_grid = {'n_estimators': [100,200,400,500,800],\n             'learning_rate': [0.1,0.05,0.3,0.7],\n             'min_samples_split': [2,4],\n             'min_samples_leaf': [0.1,0.4],\n             'max_depth': [3,4,7]\n             }\n\nestimator = GradientBoostingRegressor(random_state=42)\n\nclf = GridSearchCV(estimator=estimator, param_grid=param_grid, cv=kf, \n                   scoring='neg_mean_squared_error', n_jobs=-1)\nclf.fit(X_scaled,y)\n\nbest_estim = clf.best_estimator_\nbest_score = clf.best_score_\nbest_param = clf.best_params_\nprint(\"Best Estimator:\", best_estim)\nprint(\"Best score:\", np.sqrt(-best_score))","metadata":{"execution":{"iopub.status.busy":"2022-12-14T15:11:21.540751Z","iopub.execute_input":"2022-12-14T15:11:21.541159Z","iopub.status.idle":"2022-12-14T15:12:58.678553Z","shell.execute_reply.started":"2022-12-14T15:11:21.541125Z","shell.execute_reply":"2022-12-14T15:12:58.676335Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Regression modeling using statsmodels API","metadata":{}},{"cell_type":"code","source":"# building statsmodel regression model\nmodel = sm.OLS(y, X_scaled)\nresults = model.fit()\nprint(results.summary())","metadata":{"execution":{"iopub.status.busy":"2022-12-14T15:12:58.680817Z","iopub.execute_input":"2022-12-14T15:12:58.681267Z","iopub.status.idle":"2022-12-14T15:12:58.707849Z","shell.execute_reply.started":"2022-12-14T15:12:58.681224Z","shell.execute_reply":"2022-12-14T15:12:58.706415Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"shap_tree = shap.TreeExplainer(bgt)\nshap_values = shap_tree.shap_values(X_test)\nshap.summary_plot(shap_values, X_test)","metadata":{"execution":{"iopub.status.busy":"2022-12-14T15:12:58.710325Z","iopub.execute_input":"2022-12-14T15:12:58.710845Z","iopub.status.idle":"2022-12-14T15:12:59.361250Z","shell.execute_reply.started":"2022-12-14T15:12:58.710799Z","shell.execute_reply":"2022-12-14T15:12:59.360285Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"shap.summary_plot(shap_values, X_test, plot_type='bar')","metadata":{"execution":{"iopub.status.busy":"2022-12-14T15:12:59.362388Z","iopub.execute_input":"2022-12-14T15:12:59.363103Z","iopub.status.idle":"2022-12-14T15:12:59.668411Z","shell.execute_reply.started":"2022-12-14T15:12:59.363068Z","shell.execute_reply":"2022-12-14T15:12:59.667262Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Building ML models using sklearn pipeline object to improve RMSE score","metadata":{}},{"cell_type":"code","source":"# repeated kfold to \ncv = RepeatedKFold(n_splits= 5, n_repeats = 3, random_state = 1)\nfs_info_v0 = SelectKBest(score_func = mutual_info_regression)\n\n# define pipeline object\npipe_rf = Pipeline([\n    ('sel', fs_info_v0), \n    ('model', RandomForestRegressor(random_state=1))\n])\n\npipe_xgb = Pipeline([\n    ('sel', fs_info_v0), \n    ('model', XGBRegressor(random_state=1))\n])\n","metadata":{"execution":{"iopub.status.busy":"2022-12-14T15:12:59.670269Z","iopub.execute_input":"2022-12-14T15:12:59.671065Z","iopub.status.idle":"2022-12-14T15:12:59.680228Z","shell.execute_reply.started":"2022-12-14T15:12:59.671018Z","shell.execute_reply":"2022-12-14T15:12:59.678670Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# prepare the parameters grid\ngrid_params_rf = [{'sel__k': [i for i in range(X_train.shape[1]-6, X_train.shape[1]-4)],\n                   'model__max_depth': [15, 18, 10],\n                   'model__min_samples_split': [15, 18, 10],\n                   'model__n_estimators': [100,200,400,500]\n                  }]\n\ngrid_params_xgb = [{'sel__k': [i for i in range(X_train.shape[1]-6, X_train.shape[1]-4)],\n                    'model__max_depth': [9,12],\n                    'model__min_child_weight': [7,8],\n                    'model__subsample': [i/10. for i in range(9,11)]\n                   }]","metadata":{"execution":{"iopub.status.busy":"2022-12-14T15:12:59.681644Z","iopub.execute_input":"2022-12-14T15:12:59.682356Z","iopub.status.idle":"2022-12-14T15:12:59.698361Z","shell.execute_reply.started":"2022-12-14T15:12:59.682319Z","shell.execute_reply":"2022-12-14T15:12:59.697008Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# set up the gridsearchCV objects\nRF = GridSearchCV(estimator=pipe_rf,\n            param_grid=grid_params_rf,\n            scoring='neg_mean_absolute_error',\n            cv=cv, \n            n_jobs= -1)\n\nXGB = GridSearchCV(estimator=pipe_xgb,\n            param_grid=grid_params_xgb,\n            scoring='neg_mean_absolute_error',\n            cv=cv, \n            n_jobs= -1)\n\n# list of regression models\ngrids = [RF,XGB]\n\n# Creating a dict for our reference\ngrid_dict = { \n        0: 'Random Forest',\n        1: 'XGBoost'\n    }","metadata":{"execution":{"iopub.status.busy":"2022-12-14T15:12:59.699956Z","iopub.execute_input":"2022-12-14T15:12:59.700285Z","iopub.status.idle":"2022-12-14T15:12:59.711050Z","shell.execute_reply.started":"2022-12-14T15:12:59.700257Z","shell.execute_reply":"2022-12-14T15:12:59.710126Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Start form initial scaled model: X_train and X_test, y_train and y_test\ndef extract_best_model(grids: list, grid_dict: dict):\n    print('Performing model optimizations...')\n    least_mae = 270817\n    best_regr = 0\n    best_gs = ''\n    for idx, gs in enumerate(grids):\n        print('\\nEstimator: %s' % grid_dict[idx])\n        gs.fit(X_train, y_train)\n        print('Best Config: %s' % gs.best_params_)\n        # Best training data accuracy\n        print('Best MAE: %.3f' % gs.best_score_)\n        # Predict on test data with best params\n        y_pred_v0 = gs.predict(X_test)\n        # Test data accuracy of model with best params\n        print('Test set mean absolute error for best params: %.3f ' % mean_absolute_error(y_test, y_pred_v0))\n        print('Test set root mean squared error for best params: %.3f ' % np.sqrt(mean_absolute_error(y_test, y_pred_v0)))\n        \n        # Track best (least test error) model\n        if mean_absolute_error(y_test, y_pred_v0) < least_mae:\n            least_mae = mean_absolute_error(y_test, y_pred_v0)\n            best_gs = gs\n            best_regr = idx\n    print('\\nClassifier with least test set MAE: %s' % grid_dict[best_regr])\n\n    \n    return (grid_dict[best_regr], best_gs, least_mae)","metadata":{"execution":{"iopub.status.busy":"2022-12-14T15:12:59.712376Z","iopub.execute_input":"2022-12-14T15:12:59.712712Z","iopub.status.idle":"2022-12-14T15:12:59.726549Z","shell.execute_reply.started":"2022-12-14T15:12:59.712684Z","shell.execute_reply":"2022-12-14T15:12:59.725329Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# run the pipeline and print the results\nbest_model_name_v0, best_model_v0, least_mae_v0 = extract_best_model(grids= grids, grid_dict = grid_dict)\n\nprint(f\"Best Model: {best_model_name_v0}\")\nprint(f\"Error Rate: {least_mae_v0}\")\nprint(best_model_v0)","metadata":{"execution":{"iopub.status.busy":"2022-12-14T15:12:59.732125Z","iopub.execute_input":"2022-12-14T15:12:59.733026Z","iopub.status.idle":"2022-12-14T15:17:11.901960Z","shell.execute_reply.started":"2022-12-14T15:12:59.732971Z","shell.execute_reply":"2022-12-14T15:17:11.901025Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Flask-deplyment files","metadata":{}},{"cell_type":"code","source":"X_train_n = X_train.drop('n_cluster', axis=1)\nX_test_n = X_test.drop('n_cluster', axis=1)","metadata":{"execution":{"iopub.status.busy":"2022-12-14T16:28:32.680098Z","iopub.execute_input":"2022-12-14T16:28:32.680620Z","iopub.status.idle":"2022-12-14T16:28:32.689017Z","shell.execute_reply.started":"2022-12-14T16:28:32.680575Z","shell.execute_reply":"2022-12-14T16:28:32.687832Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# train flask deployment\nxgb_model = XGBRegressor(max_depth=9, min_child_weight=7, subsample=1.0)\nxgb_model.fit(X_train_n, y_train)\npr = xgb_model.predict(X_test_n)\nerr = mean_absolute_error(y_test, pr)\nrmse_n = np.sqrt(mean_squared_error(y_test, pr))\nprint(err)","metadata":{"execution":{"iopub.status.busy":"2022-12-14T16:28:59.239613Z","iopub.execute_input":"2022-12-14T16:28:59.240152Z","iopub.status.idle":"2022-12-14T16:29:00.287932Z","shell.execute_reply.started":"2022-12-14T16:28:59.240110Z","shell.execute_reply":"2022-12-14T16:29:00.286841Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(rmse_n)","metadata":{"execution":{"iopub.status.busy":"2022-12-14T16:29:06.506666Z","iopub.execute_input":"2022-12-14T16:29:06.507094Z","iopub.status.idle":"2022-12-14T16:29:06.513032Z","shell.execute_reply.started":"2022-12-14T16:29:06.507053Z","shell.execute_reply":"2022-12-14T16:29:06.511619Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_test_n.columns","metadata":{"execution":{"iopub.status.busy":"2022-12-14T16:29:32.301587Z","iopub.execute_input":"2022-12-14T16:29:32.302642Z","iopub.status.idle":"2022-12-14T16:29:32.311930Z","shell.execute_reply.started":"2022-12-14T16:29:32.302587Z","shell.execute_reply":"2022-12-14T16:29:32.309546Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"joblib.dump(xgb_model, 'wbb_xgb_model2.joblib')","metadata":{"execution":{"iopub.status.busy":"2022-12-14T16:29:42.743887Z","iopub.execute_input":"2022-12-14T16:29:42.744333Z","iopub.status.idle":"2022-12-14T16:29:42.762229Z","shell.execute_reply.started":"2022-12-14T16:29:42.744298Z","shell.execute_reply":"2022-12-14T16:29:42.761164Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Conclusions\n\n- Final modeling pipeline shows XGBoost regressor with least RMSE score of `18` and MAE of `359`\n- We have best parameters as well as best fitted model as well.","metadata":{}}]}